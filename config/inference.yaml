# =============================================================================
# Inference Server Configuration
# =============================================================================
# Centralized config for all GGUF model inference servers.
# These values are optimized for GitHub Actions ARM runners (4 vCPU, 16GB RAM).
#
# Models inherit from 'defaults' unless they have specific overrides.
# =============================================================================

defaults:
  n_ctx: 4096        # Context window size (tokens)
  n_threads: 4       # CPU threads - matches runner vCPU count
  n_batch: 256       # Batch size for prompt processing
  max_concurrent: 2  # Max parallel requests per instance
  kv_cache_quant: true  # Q8_0 KV-cache quantization (reduces memory ~30%)

# Model-specific overrides
# Only specify values that differ from defaults
models:
  llama:
    # Uses defaults - Llama 3.2 3B is lightweight

  gemma:
    # Gemma 3 12B IT - bump context to 8K for better chat quality
    n_ctx: 8192

  mistral:
    # Uses defaults - Mistral 7B is efficient

  phi:
    # Uses defaults - Phi-3.5 Mini is small and fast

  qwen:
    # Uses defaults - Qwen 2.5 7B is well-optimized

  gpt-oss:
    # Uses defaults - GPT-2 is tiny

  deepseek-r1qwen:
    # Uses defaults - 7B distilled model

  functiongemma:
    # Uses defaults - 270M parameter model, very fast

  smollm3:
    # SmolLM3-3B: Supports /think and /no_think reasoning modes
    # Small and fast with strong benchmarks
    n_ctx: 4096          # Default context, can extend to 64K
    n_batch: 512         # Good prompt processing
    max_concurrent: 3    # Can handle multiple requests

  lfm2:
    # LFM2 2.6B: ChatML-like template, 32K native context
    # max_concurrent=1 workaround for KV cache bug (llama.cpp #16278)
    # Reduced n_ctx to 2048 to avoid decode errors on limited hardware
    n_ctx: 2048
    max_concurrent: 1


  nanbeige:
    # Nanbeige4-3B-Thinking: SPEED-OPTIMIZED config
    # Smaller context + Q4 quant = maximum throughput
    n_ctx: 2048          # Minimal context for speed
    n_batch: 512         # Fast prompt processing
    max_concurrent: 4    # Maximum parallel requests

  nemotron:
    # 30B MoE model with IQ2_M quantization (~8GB) - OPTIMIZED CPU CONFIG
    # Uses q8_0 KV cache, cont-batching, flash-attn for best CPU performance
    n_ctx: 512           # Small but usable context
    n_threads: 4         # Use all vCPUs
    n_batch: 256         # Larger batch for prompt processing
    max_concurrent: 1    # Single request only (MoE overhead)

  rnj:
    # 8B model with unique architecture
    n_ctx: 2048
    n_batch: 512
    max_concurrent: 3
