# =============================================================================
# Inference Server Configuration
# =============================================================================
# Centralized config for all GGUF model inference servers.
# These values are optimized for GitHub Actions ARM runners (4 vCPU, 16GB RAM).
#
# Models inherit from 'defaults' unless they have specific overrides.
# =============================================================================

defaults:
  n_ctx: 4096        # Context window size (tokens)
  n_threads: 4       # CPU threads - matches runner vCPU count
  n_batch: 256       # Batch size for prompt processing
  max_concurrent: 2  # Max parallel requests per instance
  kv_cache_quant: false  # Q8_0 KV-cache quantization (reduces memory ~30%)

# Model-specific overrides
# Only specify values that differ from defaults
models:
  llama:
    # Uses defaults - Llama 3.2 3B is lightweight

  gemma:
    # Gemma 3 12B IT - bump context to 8K for better chat quality
    n_ctx: 8192

  mistral:
    # Uses defaults - Mistral 7B is efficient

  phi:
    # Uses defaults - Phi-3.5 Mini is small and fast

  qwen:
    # Uses defaults - Qwen 2.5 7B is well-optimized

  gpt-oss:
    # Uses defaults - GPT-2 is tiny

  deepseek-r1qwen:
    # Uses defaults - 7B distilled model

  functiongemma:
    # Uses defaults - 270M parameter model, very fast

  smollm3:
    # SmolLM3-3B: Supports /think and /no_think reasoning modes
    # Small and fast with strong benchmarks
    n_ctx: 4096          # Default context, can extend to 64K
    n_batch: 512         # Good prompt processing
    max_concurrent: 3    # Can handle multiple requests

  nanbeige:
    # Nanbeige4-3B-Thinking: SPEED-OPTIMIZED config
    # Smaller context + Q4 quant = maximum throughput
    # Note: kv_cache_quant disabled - requires flash_attn which isn't in CPU build
    n_ctx: 2048          # Minimal context for speed
    n_batch: 512         # Fast prompt processing
    max_concurrent: 4    # Maximum parallel requests

  nemotron:
    # 30B MoE model with IQ2_M quantization (~8GB) - ULTRA-MINIMAL CONFIG
    # Model is too large for efficient CPU inference, bare minimum settings
    n_ctx: 256           # Bare minimum context
    n_threads: 2         # Reduce memory contention (bottleneck is bandwidth)
    n_batch: 128         # Smallest practical batch
    max_concurrent: 1    # Single request only

  rnj:
    # 8B model with unique architecture
    n_ctx: 2048
    n_batch: 512
    max_concurrent: 3
