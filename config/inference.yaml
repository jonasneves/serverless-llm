# =============================================================================
# Inference Server Configuration
# =============================================================================
# Centralized config for all GGUF model inference servers.
# These values are optimized for GitHub Actions ARM runners (4 vCPU, 16GB RAM).
#
# Models inherit from 'defaults' unless they have specific overrides.
# =============================================================================

defaults:
  n_ctx: 4096        # Context window size (tokens)
  n_threads: 4       # CPU threads - matches runner vCPU count
  n_batch: 256       # Batch size for prompt processing
  max_concurrent: 2  # Max parallel requests per instance
  kv_cache_quant: true  # Q8_0 KV-cache quantization (reduces memory ~30%)

# Model-specific overrides
# Only specify values that differ from defaults
models:
  llama:
    display_name: "Llama 3.2-3B"
    model_dir: "llama-inference"
    model_repo: "unsloth/Llama-3.2-3B-Instruct-GGUF"
    model_file: "Llama-3.2-3B-Instruct-Q4_K_M.gguf"

  gemma:
    display_name: "Gemma 3 12B"
    model_dir: "gemma-inference"
    model_repo: "unsloth/gemma-3-12b-it-GGUF"
    model_file: "gemma-3-12b-it-Q4_K_M.gguf"
    n_ctx: 8192

  mistral:
    display_name: "Mistral 7B"
    model_dir: "mistral-inference"
    model_repo: "bartowski/Mistral-7B-Instruct-v0.3-GGUF"
    model_file: "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"

  phi:
    display_name: "Phi-3 Mini"
    model_dir: "phi-inference"
    model_repo: "bartowski/Phi-3-mini-4k-instruct-GGUF"
    model_file: "Phi-3-mini-4k-instruct-Q4_K_M.gguf"

  qwen:
    display_name: "Qwen 3-4B"
    model_dir: "qwen-inference"
    model_repo: "unsloth/Qwen3-4B-Instruct-2507-GGUF"
    model_file: "Qwen3-4B-Instruct-2507-Q4_K_M.gguf"

  gptoss:
    display_name: "GPT-OSS 20B"
    model_dir: "gpt-oss-inference"
    model_repo: "unsloth/gpt-oss-20b-GGUF"
    model_file: "gpt-oss-20b-Q6_K.gguf"

  r1qwen:
    display_name: "DeepSeek R1 1.5B"
    model_dir: "deepseek-r1qwen-inference"
    model_repo: "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF"
    model_file: "DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf"

  functiongemma:
    display_name: "FunctionGemma 270M"
    model_dir: "functiongemma-inference"
    model_repo: "unsloth/functiongemma-270m-it-GGUF"
    model_file: "functiongemma-270m-it-Q8_0.gguf"

  smollm3:
    display_name: "SmolLM3 3B"
    model_dir: "smollm3-inference"
    model_repo: "unsloth/SmolLM3-3B-GGUF"
    model_file: "SmolLM3-3B-Q4_K_M.gguf"
    n_ctx: 4096
    n_batch: 512
    max_concurrent: 3

  lfm2:
    display_name: "LFM2 2.6B"
    model_dir: "lfm2-inference"
    model_repo: "LiquidAI/LFM2-2.6B-GGUF"
    model_file: "LFM2-2.6B-Q4_K_M.gguf"
    n_ctx: 2048
    max_concurrent: 1

  "lfm2.5":
    display_name: "LFM2.5 1.2B"
    model_dir: "lfm2.5-inference"
    model_repo: "LiquidAI/LFM2.5-1.2B-Instruct-GGUF"
    model_file: "LFM2.5-1.2B-Instruct-Q4_K_M.gguf"
    n_ctx: 8192
    n_batch: 512
    max_concurrent: 2

  nanbeige:
    display_name: "Nanbeige4-3B Thinking"
    model_dir: "nanbeige-inference"
    model_repo: "bartowski/Nanbeige_Nanbeige4-3B-Thinking-2511-GGUF"
    model_file: "Nanbeige_Nanbeige4-3B-Thinking-2511-Q4_K_M.gguf"
    n_ctx: 2048
    n_batch: 512
    max_concurrent: 4

  nemotron:
    display_name: "Nemotron-3 Nano 30B"
    model_dir: "nemotron-inference"
    model_repo: "unsloth/Nemotron-3-Nano-30B-A3B-GGUF"
    model_file: "Nemotron-3-Nano-30B-A3B-UD-IQ2_M.gguf"
    n_ctx: 512
    n_threads: 4
    n_batch: 256
    max_concurrent: 1

  rnj:
    display_name: "RNJ-1"
    model_dir: "rnj-inference"
    model_repo: "EssentialAI/rnj-1-instruct-GGUF"
    model_file: "Rnj-1-Instruct-8B-Q4_K_M.gguf"
    n_ctx: 2048
    n_batch: 512
    max_concurrent: 3
