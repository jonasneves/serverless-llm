# Requirements for llama-server wrapper models (rnj, lfm2)
# These models run llama-server directly instead of llama-cpp-python bindings
# due to architecture-specific requirements (rnj1, lfm2)

fastapi>=0.104.1
uvicorn[standard]>=0.24.0
huggingface-hub>=0.20.0
pydantic>=2.0.0
httpx>=0.25.0
