version: '3.8'

# Complete Stack: All inference servers + chat interface
# Demonstrates full architecture with separated lightweight services

services:
  # ============================================
  # INFERENCE SERVERS
  # ============================================

  phi-inference:
    build: ./app/phi-inference
    environment:
      - MODEL_REPO=bartowski/Phi-3-mini-4k-instruct-GGUF
      - MODEL_FILE=Phi-3-mini-4k-instruct-Q4_K_M.gguf
      - PORT=8000
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - phi-cache:/app/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      start_period: 120s
    networks:
      - llm-network

  qwen-inference:
    build: ./app/qwen-inference
    environment:
      - MODEL_REPO=bartowski/Qwen2.5-7B-Instruct-GGUF
      - MODEL_FILE=Qwen2.5-7B-Instruct-Q4_K_M.gguf
      - PORT=8001
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - qwen-cache:/app/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      start_period: 120s
    networks:
      - llm-network

  llama-inference:
    build: ./app/llama-inference
    environment:
      - MODEL_REPO=bartowski/Llama-3.2-3B-Instruct-GGUF
      - MODEL_FILE=Llama-3.2-3B-Instruct-Q4_K_M.gguf
      - PORT=8002
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - llama-cache:/app/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      start_period: 120s
    networks:
      - llm-network

  mistral-inference:
    build: ./app/mistral-inference
    environment:
      - MODEL_REPO=bartowski/Mistral-7B-Instruct-v0.3-GGUF
      - MODEL_FILE=Mistral-7B-Instruct-v0.3-Q4_K_M.gguf
      - PORT=8003
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - mistral-cache:/app/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      start_period: 120s
    networks:
      - llm-network

  gemma-inference:
    build: ./app/gemma-inference
    environment:
      - MODEL_REPO=bartowski/gemma-2-2b-it-GGUF
      - MODEL_FILE=gemma-2-2b-it-Q4_K_M.gguf
      - PORT=8004
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - gemma-cache:/app/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      start_period: 120s
    networks:
      - llm-network

  # ============================================
  # CHAT INTERFACE
  # ============================================

  chat-server:
    build: ./app/chat-interface
    environment:
      - PORT=8080
      - PHI_API_URL=http://phi-inference:8000
      - QWEN_API_URL=http://qwen-inference:8001
      - LLAMA_API_URL=http://llama-inference:8002
      - MISTRAL_API_URL=http://mistral-inference:8003
      - GEMMA_API_URL=http://gemma-inference:8004
      - GH_MODELS_TOKEN=${GH_MODELS_TOKEN}
    ports:
      - "8080:8080"
    depends_on:
      phi-inference:
        condition: service_healthy
      qwen-inference:
        condition: service_healthy
      llama-inference:
        condition: service_healthy
      mistral-inference:
        condition: service_healthy
      gemma-inference:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
    networks:
      - llm-network

  # ============================================
  # LIGHTWEIGHT SIDECARS (ubuntu:24.04 ~30MB each)
  # ============================================

  chat-cloudflared:
    build: ./docker/cloudflared
    command: ["tunnel", "--no-autoupdate", "run", "--token", "${CLOUDFLARE_TUNNEL_TOKEN_CHAT}"]
    depends_on:
      chat-server:
        condition: service_healthy
    networks:
      - llm-network

  phi-cloudflared:
    build: ./docker/cloudflared
    command: ["tunnel", "--no-autoupdate", "run", "--token", "${CLOUDFLARE_TUNNEL_TOKEN_PHI}"]
    depends_on:
      phi-inference:
        condition: service_healthy
    networks:
      - llm-network

  qwen-cloudflared:
    build: ./docker/cloudflared
    command: ["tunnel", "--no-autoupdate", "run", "--token", "${CLOUDFLARE_TUNNEL_TOKEN_QWEN}"]
    depends_on:
      qwen-inference:
        condition: service_healthy
    networks:
      - llm-network

  # Central health monitor for all services
  health-monitor:
    build: ./docker/health-monitor
    environment:
      - SERVER_URL=http://chat-server:8080/health
      - CHECK_INTERVAL=30
    depends_on:
      - chat-server
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge

volumes:
  phi-cache:
  qwen-cache:
  llama-cache:
  mistral-cache:
  gemma-cache:

# Usage:
# docker-compose -f docker-compose.all.yml up -d
# docker-compose -f docker-compose.all.yml ps
# docker-compose -f docker-compose.all.yml logs -f chat-server
# docker-compose -f docker-compose.all.yml down
