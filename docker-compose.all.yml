# Full stack docker-compose for local development
# Usage: docker-compose -f docker-compose.all.yml up -d
#
# Note: This requires all model images to be built first.
# For production, use individual compose files or GitHub Actions.

version: '3.8'

services:
  chat-interface:
    build:
      context: ./app/chat-interface
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
      - QWEN_API_URL=http://qwen:8000
      - PHI_API_URL=http://phi:8000
      - LLAMA_API_URL=http://llama:8000
      - MISTRAL_API_URL=http://mistral:8000
      - GEMMA_API_URL=http://gemma:8000
      - R1QWEN_API_URL=http://r1qwen:8000
      - GH_MODELS_TOKEN=${GH_MODELS_TOKEN}
    depends_on:
      - qwen
    networks:
      - llm-network
    restart: unless-stopped

  qwen:
    build:
      context: ./app/qwen-inference
      dockerfile: Dockerfile
    ports:
      - "8001:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    networks:
      - llm-network
    restart: unless-stopped

  phi:
    build:
      context: ./app/phi-inference
      dockerfile: Dockerfile
    ports:
      - "8002:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    networks:
      - llm-network
    restart: unless-stopped

  llama:
    build:
      context: ./app/llama-inference
      dockerfile: Dockerfile
    ports:
      - "8003:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    networks:
      - llm-network
    restart: unless-stopped

  r1qwen:
    build:
      context: ./app/deepseek-r1qwen-inference
      dockerfile: Dockerfile
    ports:
      - "8004:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    networks:
      - llm-network
    restart: unless-stopped

  mistral:
    build:
      context: ./app/mistral-inference
      dockerfile: Dockerfile
    ports:
      - "8005:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    networks:
      - llm-network
    restart: unless-stopped

  gemma:
    build:
      context: ./app/gemma-inference
      dockerfile: Dockerfile
    ports:
      - "8006:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    networks:
      - llm-network
    restart: unless-stopped

networks:
  llm-network:
    driver: bridge
