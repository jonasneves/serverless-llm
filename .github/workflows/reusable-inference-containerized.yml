name: Reusable Containerized Inference

on:
  workflow_call:
    inputs:
      model_name:
        required: true
        type: string
      model_dir:
        required: true
        type: string
      model_repo:
        required: true
        type: string
      model_file:
        required: true
        type: string
      cache_key_prefix:
        required: true
        type: string
      tunnel_secret_name:
        required: true
        type: string
      restart_event_type:
        required: true
        type: string
      workflow_file:
        required: true
        type: string
      duration_hours:
        type: string
        default: '5'
      auto_restart:
        type: boolean
        default: true
      instances:
        type: string
        default: '1'
      port:
        type: string
        default: '8000'
      n_ctx:
        type: string
        required: false
        description: 'Context window size (leave empty to use model default)'
      n_threads:
        type: string
        required: false
        description: 'Number of CPU threads (leave empty to use model default)'
      n_batch:
        type: string
        required: false
        description: 'Batch size for processing (leave empty to use model default)'
    secrets:
      hf_token:
        required: true
      tunnel_token:
        required: true
      workflow_pat:
        required: true

jobs:
  inference:
    name: ${{ inputs.model_name }} #${{ matrix.instance }}
    runs-on: ubuntu-latest
    timeout-minutes: 350
    permissions:
      actions: write
      contents: read
      packages: read
    strategy:
      matrix:
        instance: ${{ fromJson(inputs.instances == '3' && '[1,2,3]' || inputs.instances == '2' && '[1,2]' || '[1]') }}
      fail-fast: false

    steps:
      - name: Find previous workflow
        id: find_previous
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          API_RESPONSE=$(curl -s \
            -H "Authorization: token $GH_TOKEN" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/${{ github.repository }}/actions/workflows/${{ inputs.workflow_file }}/runs?status=in_progress")

          PREVIOUS_RUN_ID=$(echo "$API_RESPONSE" | jq -r ".workflow_runs[] | select(.id < ${{ github.run_id }}) | .id" | head -1)

          if [ -n "$PREVIOUS_RUN_ID" ] && [ "$PREVIOUS_RUN_ID" != "null" ]; then
            echo "previous_run_id=$PREVIOUS_RUN_ID" >> $GITHUB_OUTPUT
          else
            echo "previous_run_id=" >> $GITHUB_OUTPUT
          fi

      - uses: actions/checkout@v4

      - name: Setup dynamic swap (zram + sparse file)
        run: |
          echo "Setting up hybrid dynamic swap..."

          # Method 1: zram (primary, 4GB compressed ≈ 12GB effective)
          # Fast, no disk space, perfect for hot data
          if ! lsmod | grep -q zram; then
            sudo modprobe zram
            echo lz4 | sudo tee /sys/block/zram0/comp_algorithm > /dev/null
            echo 4G | sudo tee /sys/block/zram0/disksize > /dev/null
            sudo mkswap /dev/zram0
            sudo swapon -p 100 /dev/zram0
            echo "✓ zram swap enabled (4GB compressed, ~12GB effective, 0 disk space)"
          fi

          # Method 2: Sparse file backup (8GB max, grows from 0 as needed)
          # Only uses disk space when zram is full
          if [ ! -f /swapfile ]; then
            sudo truncate -s 8G /swapfile
            sudo chmod 600 /swapfile
            sudo mkswap /swapfile
            sudo swapon -p 10 /swapfile
            echo "✓ Sparse swap file created (0 → 8GB dynamic, lower priority)"
          fi

          # Optimize for model loading and inference
          sudo sysctl -w vm.swappiness=40 > /dev/null           # Balance RAM/swap for builds
          sudo sysctl -w vm.vfs_cache_pressure=50 > /dev/null   # Keep model cache longer

          # Show configuration
          echo ""
          echo "Memory configuration:"
          free -h
          echo ""
          echo "Swap devices:"
          swapon --show
          echo ""
          echo "Actual disk usage of sparse file:"
          sudo du -h /swapfile 2>/dev/null || echo "  (will grow as needed)"

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull cloudflared from GHCR
        run: |
          IMAGE="ghcr.io/${{ github.repository_owner }}/serverless-llm/cloudflared:latest"
          echo "Pulling $IMAGE"
          docker pull "$IMAGE"
          docker tag "$IMAGE" "cloudflared:latest"
          echo "Successfully pulled cloudflared from GHCR"

      - name: Pull inference image from GHCR
        run: |
          IMAGE="ghcr.io/${{ github.repository_owner }}/serverless-llm/${{ inputs.model_dir }}:latest"
          echo "Pulling $IMAGE"
          docker pull "$IMAGE"
          docker tag "$IMAGE" "${{ inputs.model_name }}-inference:latest"
          echo "Successfully pulled image from GHCR"

      - name: Create network
        run: docker network create inference-network

      - name: Start inference server
        env:
          MODEL_REPO: ${{ inputs.model_repo }}
          MODEL_FILE: ${{ inputs.model_file }}
          PORT: ${{ inputs.port }}
          HF_TOKEN: ${{ secrets.hf_token }}
          N_CTX: ${{ inputs.n_ctx }}
          N_THREADS: ${{ inputs.n_threads }}
          N_BATCH: ${{ inputs.n_batch }}
        run: |
          # Build docker run command with optional parameters
          CMD="docker run -d \
            --name inference-server \
            --network inference-network \
            -p ${{ inputs.port }}:${{ inputs.port }} \
            -e MODEL_REPO=\"$MODEL_REPO\" \
            -e MODEL_FILE=\"$MODEL_FILE\" \
            -e PORT=\"$PORT\" \
            -e HF_TOKEN=\"$HF_TOKEN\" \
            -e HF_HOME=/app/.cache/huggingface"

          # Add N_CTX if provided
          if [ -n "$N_CTX" ]; then
            CMD="$CMD -e N_CTX=\"$N_CTX\""
          fi

          # Add N_THREADS if provided
          if [ -n "$N_THREADS" ]; then
            CMD="$CMD -e N_THREADS=\"$N_THREADS\""
          fi

          # Add N_BATCH if provided
          if [ -n "$N_BATCH" ]; then
            CMD="$CMD -e N_BATCH=\"$N_BATCH\""
          fi

          CMD="$CMD -v \$HOME/.cache/huggingface:/app/.cache/huggingface \
            ${{ inputs.model_name }}-inference:latest"

          echo "Starting inference server with command:"
          echo "$CMD"
          eval $CMD

          # Health check
          HEALTHY=false
          for i in {1..30}; do
            if ! docker ps | grep -q inference-server; then
              echo "Container stopped"
              docker logs inference-server
              exit 1
            fi

            RESPONSE=$(curl -s http://localhost:${{ inputs.port }}/health || echo "")
            if echo "$RESPONSE" | grep -q "healthy"; then
              echo "Server ready"
              HEALTHY=true
              break
            fi

            echo "Waiting... ($i/30)"
            sleep 10
          done

          if [ "$HEALTHY" = "false" ]; then
            docker logs inference-server
            exit 1
          fi

      - name: Start cloudflared
        env:
          TUNNEL_TOKEN: ${{ secrets.tunnel_token }}
        run: |
          docker run -d \
            --name cloudflared \
            --network host \
            cloudflared:latest \
            tunnel --no-autoupdate run --token "$TUNNEL_TOKEN"

          sleep 10
          docker logs cloudflared | head -20

      - name: Cancel previous workflow
        if: steps.find_previous.outputs.previous_run_id != ''
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          curl -s -X POST \
            -H "Authorization: token $GH_TOKEN" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ steps.find_previous.outputs.previous_run_id }}/cancel"

          echo "Graceful handoff: waiting 60s for previous instance to shutdown"
          sleep 60

      - name: Monitor
        env:
          DURATION_HOURS: ${{ inputs.duration_hours }}
          AUTO_RESTART: ${{ inputs.auto_restart }}
          PORT: ${{ inputs.port }}
        run: |
          cleanup() {
            docker stop inference-server cloudflared 2>/dev/null || true
            docker rm inference-server cloudflared 2>/dev/null || true
            docker network rm inference-network 2>/dev/null || true
            exit 0
          }
          trap cleanup SIGTERM SIGINT

          DURATION=$((DURATION_HOURS * 3600))
          RESTART_THRESHOLD=$((DURATION - 360))
          START_TIME=$(date +%s)
          RESTART_TRIGGERED=false

          while true; do
            ELAPSED=$(($(date +%s) - START_TIME))
            REMAINING=$((DURATION - ELAPSED))

            SERVER_STATUS=$(docker inspect -f '{{.State.Status}}' inference-server 2>/dev/null || echo "stopped")
            TUNNEL_STATUS=$(docker inspect -f '{{.State.Status}}' cloudflared 2>/dev/null || echo "stopped")

            printf "[%s] Server: %s | Tunnel: %s | %dm elapsed | %dm remaining\n" \
              "$(date '+%H:%M:%S')" "$SERVER_STATUS" "$TUNNEL_STATUS" "$((ELAPSED/60))" "$((REMAINING/60))"

            [ "$SERVER_STATUS" != "running" ] && docker start inference-server
            [ "$TUNNEL_STATUS" != "running" ] && docker start cloudflared

            # Auto-restart
            if [ "$ELAPSED" -gt "$RESTART_THRESHOLD" ] && [ "$RESTART_TRIGGERED" = "false" ] && [ "${{ matrix.instance }}" = "1" ]; then
              if [ "$AUTO_RESTART" = "true" ]; then
                PAT="${{ secrets.workflow_pat }}"
                if [ -n "$PAT" ]; then
                  curl -sX POST \
                    -H "Authorization: Bearer $PAT" \
                    https://api.github.com/repos/${{ github.repository }}/dispatches \
                    -d "{\"event_type\":\"${{ inputs.restart_event_type }}\",\"client_payload\":{\"duration_hours\":\"$DURATION_HOURS\",\"auto_restart\":$AUTO_RESTART,\"instances\":\"${{ inputs.instances }}\"}}"

                  echo "Waiting 90s for new instance to start"
                  sleep 90
                fi
                RESTART_TRIGGERED=true
              fi
            fi

            [ $ELAPSED -gt $DURATION ] && break
            sleep 30
          done

          cleanup

      - name: Show logs on failure
        if: failure()
        run: |
          docker logs inference-server 2>&1 | tail -100
          docker logs cloudflared 2>&1 | tail -50
