name: ðŸ§ª LFM2 2.6B Exp

run-name: "LFM2 2.6B Exp â€¢ ${{ inputs.duration_hours || '5' }}h Ã— ${{ inputs.instances || '1' }}"

on:
  workflow_dispatch:
    inputs:
      instances:
        description: 'Parallel instances (1-3)'
        required: false
        default: '1'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
      duration_hours:
        description: 'Duration (max 5.5 hours)'
        required: false
        default: '5'
        type: string
      auto_restart:
        description: 'Auto-restart before timeout'
        required: false
        default: true
        type: boolean

  repository_dispatch:
    types: [restart-lfm2-inference]

permissions:
  actions: write
  contents: read
  packages: read

jobs:
  inference:
    uses: ./.github/workflows/reusable-inference-containerized.yml
    with:
      model_name: "lfm2"
      model_dir: "lfm2-inference"
      model_repo: "LiquidAI/LFM2-2.6B-Exp-GGUF"
      model_file: "LFM2-2.6B-Exp-Q4_K_M.gguf"
      cache_key_prefix: "gguf-lfm2-2.6b-exp-q4km"
      restart_event_type: "restart-lfm2-inference"
      workflow_file: "lfm2-inference.yml"
      duration_hours: ${{ github.event.client_payload.duration_hours || inputs.duration_hours || '5' }}
      auto_restart: ${{ (github.event.client_payload.auto_restart == true || github.event.client_payload.auto_restart == 'true') || (inputs.auto_restart == true || inputs.auto_restart == 'true' || inputs.auto_restart == '') }}
      instances: ${{ github.event.client_payload.instances || inputs.instances || '1' }}
    secrets:
      hf_token: ${{ secrets.HF_TOKEN }}
      tunnels_json: ${{ secrets.TUNNELS_JSON }}
      workflow_pat: ${{ secrets.WORKFLOW_PAT }}
