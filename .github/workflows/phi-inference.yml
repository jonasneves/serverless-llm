name: Phi-3 Mini

run-name: Phi-3 Mini Inference Server

on:
  workflow_dispatch:
    inputs:
      instances:
        description: 'Parallel instances (1-3)'
        required: false
        default: '1'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
      duration_hours:
        description: 'Duration (max 5.5 hours)'
        required: false
        default: '5'
        type: string
      auto_restart:
        description: 'Auto-restart before timeout'
        required: false
        default: true
        type: boolean
      n_ctx:
        description: 'Context window size'
        required: false
        default: '4096'
        type: string
      n_threads:
        description: 'CPU threads'
        required: false
        default: '2'
        type: string
      n_batch:
        description: 'Batch size'
        required: false
        default: '512'
        type: string
      max_concurrent:
        description: 'Max concurrent requests per instance'
        required: false
        default: '4'
        type: string

  repository_dispatch:
    types: [restart-phi-inference]

# ============================================================
# YAML Anchors for shared configuration
# ============================================================
x-inference-permissions: &inference-permissions
  actions: write
  contents: read
  packages: read

x-inference-with-defaults: &inference-with-defaults
  duration_hours: ${{ github.event.client_payload.duration_hours || inputs.duration_hours || '5' }}
  auto_restart: ${{ (github.event.client_payload.auto_restart == true || github.event.client_payload.auto_restart == 'true') || (inputs.auto_restart == true || inputs.auto_restart == 'true' || inputs.auto_restart == '') }}
  instances: ${{ github.event.client_payload.instances || inputs.instances || '1' }}
  n_ctx: ${{ github.event.client_payload.n_ctx || inputs.n_ctx || '8192' }}
  n_threads: ${{ github.event.client_payload.n_threads || inputs.n_threads || '2' }}
  n_batch: ${{ github.event.client_payload.n_batch || inputs.n_batch || '256' }}
  max_concurrent: ${{ github.event.client_payload.max_concurrent || inputs.max_concurrent || '4' }}

x-common-secrets: &common-secrets
  hf_token: ${{ secrets.HF_TOKEN }}
  workflow_pat: ${{ secrets.WORKFLOW_PAT }}
# ============================================================

permissions: *inference-permissions

jobs:
  inference:
    uses: ./.github/workflows/reusable-inference-containerized.yml
    with:
      # Model-specific configuration
      model_name: "phi"
      model_dir: "phi-inference"
      model_repo: "bartowski/Phi-3-mini-4k-instruct-GGUF"
      model_file: "Phi-3-mini-4k-instruct-Q4_K_M.gguf"
      cache_key_prefix: "gguf-phi3-mini-q4"
      tunnel_secret_name: "CLOUDFLARE_TUNNEL_TOKEN_PHI"
      restart_event_type: "restart-phi-inference"
      workflow_file: "phi-inference.yml"
      # Common configuration with defaults
      <<: *inference-with-defaults
    secrets:
      <<: *common-secrets
      tunnel_token: ${{ secrets.CLOUDFLARE_TUNNEL_TOKEN_PHI }}
