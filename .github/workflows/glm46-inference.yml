name: GLM-4.6

run-name: GLM-4.6 Inference Server

on:
  workflow_dispatch:
    inputs:
      duration_hours:
        description: 'How long to run (max 5.5 hours)'
        required: false
        default: '5'
        type: string
      auto_restart:
        description: 'Auto-restart before timeout'
        required: false
        default: true
        type: boolean
      instances:
        description: 'Number of parallel instances (1-3)'
        required: false
        default: '1'
        type: choice
        options:
          - '1'
          - '2'
          - '3'

  repository_dispatch:
    types: [restart-glm46-inference]

permissions:
  actions: write
  contents: read

jobs:
  inference:
    uses: ./.github/workflows/reusable-gguf-inference.yml
    with:
      model_name: "GLM-4.6"
      model_dir: "glm46-inference"
      # These are required by the reusable workflow but not used by this server
      model_repo: "zai-org/GLM-4.6"
      model_file: "N/A"
      cache_key_prefix: "proxy-glm46"
      tunnel_secret_name: "CLOUDFLARE_TUNNEL_TOKEN_GLM46"
      restart_event_type: "restart-glm46-inference"
      workflow_file: "glm46-inference.yml"
      duration_hours: ${{ inputs.duration_hours || '5' }}
      auto_restart: ${{ inputs.auto_restart != false }}
      instances: ${{ inputs.instances || '1' }}
    secrets:
      hf_token: ${{ secrets.HF_TOKEN }}
      tunnel_token: ${{ secrets.CLOUDFLARE_TUNNEL_TOKEN_GLM46 }}
      workflow_pat: ${{ secrets.WORKFLOW_PAT }}

