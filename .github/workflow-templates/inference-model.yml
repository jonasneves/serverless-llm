# ==============================================================================
# Inference Model Workflow Template
# ==============================================================================
# This is a workflow template for adding new inference model servers.
# Copy this template and replace the placeholders to onboard a new model.
# 
# Steps to add a new model:
# 1. Copy this file to ../.github/workflows/[model]-inference.yml
# 2. Replace all [[PLACEHOLDERS]] with your model's values:
#    - [[MODEL_DISPLAY_NAME]]: Human-readable name (e.g., "Llama 3.2-3B")
#    - [[MODEL_NAME]]: Short identifier (e.g., "llama")
#    - [[MODEL_DIR]]: Directory name in app/ (e.g., "llama-inference")
#    - [[MODEL_REPO]]: HuggingFace repo (e.g., "bartowski/Llama-3.2-3B-Instruct-GGUF")
#    - [[MODEL_FILE]]: GGUF filename (e.g., "Llama-3.2-3B-Instruct-Q4_K_M.gguf")
#    - [[CACHE_KEY_PREFIX]]: Cache identifier (e.g., "gguf-llama-3.2-3b-q4")
#    - [[TUNNEL_SECRET_NAME]]: Cloudflare tunnel secret (e.g., "CLOUDFLARE_TUNNEL_TOKEN_LLAMA")
#    - [[RESTART_EVENT]]: Dispatch event type (e.g., "restart-llama-inference")
#    - [[WORKFLOW_FILE]]: This file's name (e.g., "llama-inference.yml")
# 3. Set default values for n_ctx, n_batch, max_concurrent based on model size
# 4. Create the Cloudflare tunnel secret in GitHub repository settings
# 5. Optionally create a custom Dockerfile in app/[[MODEL_DIR]]/
#
# Model-specific tuning guidelines:
# - Small models (1-3B): n_ctx=4096, n_batch=512, max_concurrent=4
# - Medium models (7-9B): n_ctx=8192, n_batch=256, max_concurrent=3
# - Large models (13B+): n_ctx=8192, n_batch=128, max_concurrent=2
# ==============================================================================

name: [[MODEL_DISPLAY_NAME]]

run-name: [[MODEL_DISPLAY_NAME]] Inference Server

on:
  workflow_dispatch:
    inputs:
      instances:
        description: 'Parallel instances (1-3)'
        required: false
        default: '1'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
      duration_hours:
        description: 'Duration (max 5.5 hours)'
        required: false
        default: '5'
        type: string
      auto_restart:
        description: 'Auto-restart before timeout'
        required: false
        default: true
        type: boolean
      n_ctx:
        description: 'Context window size'
        required: false
        default: '4096'  # Adjust based on model size
        type: string
      n_threads:
        description: 'CPU threads'
        required: false
        default: '2'
        type: string
      n_batch:
        description: 'Batch size'
        required: false
        default: '512'  # Adjust based on model size
        type: string
      max_concurrent:
        description: 'Max concurrent requests per instance'
        required: false
        default: '4'  # Adjust based on model size
        type: string

  repository_dispatch:
    types: [[[RESTART_EVENT]]]

# ============================================================
# Shared configuration using YAML anchors
# ============================================================
x-common-permissions: &common-permissions
  actions: write
  contents: read
  packages: read

x-common-secrets: &common-secrets
  hf_token: ${{ secrets.HF_TOKEN }}
  workflow_pat: ${{ secrets.WORKFLOW_PAT }}

# ============================================================

permissions: *common-permissions

jobs:
  inference:
    uses: ./.github/workflows/reusable-inference-containerized.yml
    with:
      model_name: "[[MODEL_NAME]]"
      model_dir: "[[MODEL_DIR]]"
      model_repo: "[[MODEL_REPO]]"
      model_file: "[[MODEL_FILE]]"
      cache_key_prefix: "[[CACHE_KEY_PREFIX]]"
      tunnel_secret_name: "[[TUNNEL_SECRET_NAME]]"
      restart_event_type: "[[RESTART_EVENT]]"
      workflow_file: "[[WORKFLOW_FILE]]"
      duration_hours: ${{ github.event.client_payload.duration_hours || inputs.duration_hours || '5' }}
      auto_restart: ${{ (github.event.client_payload.auto_restart == true || github.event.client_payload.auto_restart == 'true') || (inputs.auto_restart == true || inputs.auto_restart == 'true' || inputs.auto_restart == '') }}
      instances: ${{ github.event.client_payload.instances || inputs.instances || '1' }}
      n_ctx: ${{ github.event.client_payload.n_ctx || inputs.n_ctx || '8192' }}
      n_threads: ${{ github.event.client_payload.n_threads || inputs.n_threads || '2' }}
      n_batch: ${{ github.event.client_payload.n_batch || inputs.n_batch || '256' }}
      max_concurrent: ${{ github.event.client_payload.max_concurrent || inputs.max_concurrent || '4' }}
    secrets:
      <<: *common-secrets
      tunnel_token: ${{ secrets.[[TUNNEL_SECRET_NAME]] }}
