version: '3.8'

# Reusable template for GGUF inference servers
# Use with model-specific .env files

services:
  # Main inference server (needs Python + llama-cpp-python)
  inference-server:
    build:
      context: ./app/${MODEL_DIR}
      dockerfile: Dockerfile
    container_name: ${MODEL_NAME}-inference
    environment:
      - MODEL_REPO=${MODEL_REPO}
      - MODEL_FILE=${MODEL_FILE}
      - N_CTX=${N_CTX:-2048}
      - N_THREADS=${N_THREADS:-2}
      - PORT=${PORT:-8000}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/app/.cache/huggingface
    ports:
      - "${PORT:-8000}:${PORT:-8000}"
    volumes:
      # Cache models between restarts
      - huggingface-cache:/app/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Model download can take time
    networks:
      - inference-network

  # Cloudflared tunnel sidecar (ultra-lightweight, ~30MB)
  cloudflared:
    build:
      context: ./docker/cloudflared
      dockerfile: Dockerfile
    container_name: ${MODEL_NAME}-cloudflared
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    command: ["tunnel", "--no-autoupdate", "run", "--token", "${CLOUDFLARE_TUNNEL_TOKEN}"]
    depends_on:
      inference-server:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - inference-network

  # Health monitor sidecar (lightweight, ~30MB)
  health-monitor:
    build:
      context: ./docker/health-monitor
      dockerfile: Dockerfile
    container_name: ${MODEL_NAME}-health-monitor
    environment:
      - SERVER_URL=http://inference-server:${PORT:-8000}/health
      - CHECK_INTERVAL=30
      - TUNNEL_ENABLED=true
    depends_on:
      - inference-server
      - cloudflared
    restart: unless-stopped
    networks:
      - inference-network

networks:
  inference-network:
    driver: bridge

volumes:
  huggingface-cache:
    driver: local

# Usage Examples:
#
# For Phi-3:
# MODEL_DIR=phi-inference MODEL_NAME=phi MODEL_REPO=bartowski/Phi-3-mini-4k-instruct-GGUF \
# MODEL_FILE=Phi-3-mini-4k-instruct-Q4_K_M.gguf CLOUDFLARE_TUNNEL_TOKEN=$PHI_TOKEN \
# docker-compose -f docker-compose.inference.yml up -d
#
# For Qwen:
# MODEL_DIR=qwen-inference MODEL_NAME=qwen MODEL_REPO=bartowski/Qwen2.5-7B-Instruct-GGUF \
# MODEL_FILE=Qwen2.5-7B-Instruct-Q4_K_M.gguf CLOUDFLARE_TUNNEL_TOKEN=$QWEN_TOKEN \
# docker-compose -f docker-compose.inference.yml up -d
