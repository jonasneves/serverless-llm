# ===========================================
# Serverless LLM Configuration
# ===========================================
# Copy this file to .env and fill in your values

# Hugging Face (for gated models)
# HF_TOKEN=your_huggingface_token

# ===========================================
# Port Configuration (local development only)
# ===========================================
# Port scheme:
#   8080      - Chat Interface (core)
#   81XX      - Small models (<7B: qwen, phi, functiongemma)
#   82XX      - Medium models (7B-30B: gemma, llama, mistral, rnj)
#   83XX      - Reasoning models (r1qwen)

CHAT_PORT=8080
QWEN_PORT=8100
PHI_PORT=8101
FUNCTIONGEMMA_PORT=8103
GEMMA_PORT=8200
LLAMA_PORT=8201
MISTRAL_PORT=8202
RNJ_PORT=8203
R1QWEN_PORT=8300

# ===========================================
# Model API URLs (for chat interface)
# ===========================================
# Public URLs exposed via Cloudflare Tunnels

# BASE_DOMAIN=your-domain.com
# QWEN_API_URL=https://qwen.your-domain.com
# PHI_API_URL=https://phi.your-domain.com
# LLAMA_API_URL=https://llama.your-domain.com
# MISTRAL_API_URL=https://mistral.your-domain.com
# GEMMA_API_URL=https://gemma.your-domain.com
# R1QWEN_API_URL=https://r1qwen.your-domain.com
# RNJ_API_URL=https://rnj.your-domain.com
# FUNCTIONGEMMA_API_URL=https://functiongemma.your-domain.com

# ===========================================
# Cloudflare Configuration
# ===========================================
# For tunnel automation (scripts/setup_tunnels.py)
# CLOUDFLARE_API_TOKEN=your_cloudflare_api_token
# CLOUDFLARE_ACCOUNT_ID=your_account_id

