# =============================================================================
# Serverless LLM - Unified Docker Compose with Profiles
# =============================================================================
# Usage:
#   docker-compose up                      # Start chat interface only
#   docker-compose --profile all up        # Start everything
#   docker-compose --profile qwen up       # Start chat + qwen
#   docker-compose --profile qwen --profile phi up  # Start chat + qwen + phi
#
# Available profiles: all, qwen, phi, llama, mistral, gemma, r1qwen, rnj, lfm2, smollm3, nanbeige, functiongemma, nemotron, gptoss
# =============================================================================

version: '3.8'

# YAML anchors for DRY configuration
x-common-env: &common-env
  HF_TOKEN: ${HF_TOKEN}

x-inference-common: &inference-common
  networks:
    - llm-network
  restart: unless-stopped
  environment:
    <<: *common-env

services:
  # =============================================================================
  # Chat Interface (always available)
  # =============================================================================
  chat:
    build:
      context: .
      dockerfile: app/chat/backend/Dockerfile
    ports:
      - "${CHAT_PORT:-8080}:8080"
    environment:
      PORT: 8080
      GH_MODELS_TOKEN: ${GH_MODELS_TOKEN}
      # Model URLs auto-configured based on service names
      QWEN_API_URL: http://qwen:8000
      PHI_API_URL: http://phi:8000
      LFM2_API_URL: http://lfm2:8000
      LLAMA_API_URL: http://llama:8000
      MISTRAL_API_URL: http://mistral:8000
      GEMMA_API_URL: http://gemma:8000
      R1QWEN_API_URL: http://r1qwen:8000
      RNJ_API_URL: http://rnj:8000
      SMOLLM3_API_URL: http://smollm3:8000
      NANBEIGE_API_URL: http://nanbeige:8000
      FUNCTIONGEMMA_API_URL: http://functiongemma:8000
      NEMOTRON_API_URL: http://nemotron:8000
      GPTOSS_API_URL: http://gptoss:8000
    networks:
      - llm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # =============================================================================
  # Inference Servers (activated by profiles)
  # =============================================================================
  
  qwen:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: qwen
    ports:
      - "${QWEN_PORT:-8100}:8000"
    profiles: ["all", "qwen"]

  phi:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: phi
    ports:
      - "${PHI_PORT:-8101}:8000"
    profiles: ["all", "phi"]

  lfm2:
    <<: *inference-common
    build:
      context: ./app/lfm2-inference
      dockerfile: Dockerfile
    ports:
      - "${LFM2_PORT:-8105}:8000"
    profiles: ["all", "lfm2"]

  llama:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: llama
    ports:
      - "${LLAMA_PORT:-8200}:8000"
    profiles: ["all", "llama"]

  mistral:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: mistral
    ports:
      - "${MISTRAL_PORT:-8202}:8000"
    profiles: ["all", "mistral"]

  gemma:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: gemma
    ports:
      - "${GEMMA_PORT:-8200}:8000"
    profiles: ["all", "gemma"]

  r1qwen:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: r1qwen
    ports:
      - "${R1QWEN_PORT:-8300}:8000"
    profiles: ["all", "r1qwen"]

  rnj:
    <<: *inference-common
    build:
      context: ./app/rnj-inference
      dockerfile: Dockerfile
    ports:
      - "${RNJ_PORT:-8203}:8000"
    profiles: ["all", "rnj"]

  smollm3:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: smollm3
    ports:
      - "${SMOLLM3_PORT:-8104}:8000"
    profiles: ["all", "smollm3"]

  functiongemma:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: functiongemma
    ports:
      - "${FUNCTIONGEMMA_PORT:-8103}:8000"
    profiles: ["all", "functiongemma"]

  nanbeige:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: nanbeige
    ports:
      - "${NANBEIGE_PORT:-8301}:8000"
    profiles: ["all", "nanbeige"]

  nemotron:
    <<: *inference-common
    build:
      context: ./app/nemotron-inference
      dockerfile: Dockerfile
    ports:
      - "${NEMOTRON_PORT:-8302}:8000"
    profiles: ["all", "nemotron"]

  gptoss:
    <<: *inference-common
    build:
      context: ./app
      dockerfile: shared/Dockerfile.inference
    environment:
      <<: *common-env
      MODEL_NAME: gptoss
    ports:
      - "${GPTOSS_PORT:-8303}:8000"
    profiles: ["all", "gptoss"]

networks:
  llm-network:
    driver: bridge
